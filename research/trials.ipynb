{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "# For cloning github repo\n",
    "\n",
    "from langchain.text_splitter import Language\n",
    "# Required for context aware splitting. Splitting the code into chunks. Each block of function or class is split into chunks of code\n",
    "# Eg: def main() --> code inside the func is split into chunks of code\n",
    "\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "# Inorder to load the github repository, genericloader is required\n",
    "\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q GitPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloning the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "#Repo.clone_from(\"https://github.com/Sainivedhana/llama\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'source': 'test_repo\\\\main.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from langchain import PromptTemplate\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.llms import CTransformers\\nfrom src.helper import *\\nfrom flask import Flask, render_template, jsonify, request\\n\\n#Load the PDF File\\nloader=DirectoryLoader(\\'data/\\',\\n                       glob=\"*.pdf\",\\n                       loader_cls=PyPDFLoader)\\n\\ndocuments=loader.load()\\n\\n#Split Text into Chunks\\ntext_splitter=RecursiveCharacterTextSplitter(\\n                                             chunk_size=500,\\n                                             chunk_overlap=50)\\ntext_chunks=text_splitter.split_documents(documents)\\n\\n\\n#Load the Embedding Model\\nembeddings=HuggingFaceEmbeddings(model_name=\\'sentence-transformers/all-MiniLM-L6-v2\\', \\n                                 model_kwargs={\\'device\\':\\'cpu\\'})\\n\\n\\n#Convert the Text Chunks into Embeddings and Create a FAISS Vector Store\\nvector_store=FAISS.from_documents(text_chunks, embeddings)\\n\\nllm=CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\\n                  model_type=\"llama\",\\n                  config={\\'max_new_tokens\\':128,\\n                          \\'temperature\\':0.01})\\n\\n\\nqa_prompt=PromptTemplate(template=template, input_variables=[\\'context\\', \\'question\\'])\\n\\n\\n\\nchain = RetrievalQA.from_chain_type(llm=llm,\\n                                   chain_type=\\'stuff\\',\\n                                   retriever=vector_store.as_retriever(search_kwargs={\\'k\\': 2}),\\n                                   return_source_documents=False,\\n                                   chain_type_kwargs={\\'prompt\\': qa_prompt})\\nuser_inp = \"Tell me about the rainfall measurement of the paper\"\\nresult = chain({\\'query\\':user_inp})\\n\\nprint(f\\'Answer: {result}\\')', metadata={'source': 'test_repo\\\\RAG.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from langchain import PromptTemplate\\nfrom langchain import LLMChain\\nfrom langchain.llms import CTransformers\\nfrom src.helpp import *\\n\\nB_INST, E_INST = \"[INST]\", \"[/INST]\"\\nB_SYS, E_SYS = \"<<SYS>>\\\\n\", \"\\\\n<</SYS>>\\\\n\\\\n\"\\n\\n\\n# instruction = \"Convert the following text from English to Hindi: \\\\n\\\\n {text}\"\\ninstruction = \"Give a proper summary of the : \\\\n\\\\n {text}\"\\n\\nprint(CUSTOM_SYSTEM_PROMPT)\\nSYSTEM_PROMPT = B_SYS + CUSTOM_SYSTEM_PROMPT + E_SYS\\ntemplate = B_INST + SYSTEM_PROMPT + instruction + E_INST\\n\\n\\nprompt = PromptTemplate(template=template, input_variables=[\"text\"])\\n\\n\\nllm = CTransformers(model=\\'model/llama-2-7b-chat.ggmlv3.q4_0.bin\\',\\n                    model_type=\\'llama\\',\\n                    config={\\'max_new_tokens\\': 128,\\n                            \\'temperature\\': 0.01}\\n                   )\\n\\n\\nLLM_Chain=LLMChain(prompt=prompt, llm=llm)\\n\\nprint(LLM_Chain.run(\\'Encyclopedia\\'))', metadata={'source': 'test_repo\\\\run_local_custom.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'LLAMA',\\n    version= '0.0.0',\\n    author= 'Sainivedhana',\\n    author_email= 'sainivy08@gmail.com',\\n    packages= find_packages(),\\n    install_requires = []\\n\\n)\", metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/run_local.py\",\\n    \"src/helper.py\",\\n    \"model/instruction.txt\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"main.py\",\\n    \"research/trials.ipynb\",\\n    \"src/help.py\"\\n]\\n\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n\\n    if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")', metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# This file contains the default and custom system prompt. This needs to be imported in main file to access the prompt\\n\\nDEFAULT_SYSTEM_PROMPT=\"\"\"\\\\\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \\nPlease ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of \\nanswering something not correct. If you don\\'t know the answer to a question,\\nplease don\\'t share false information.\"\"\"\\n\\nCUSTOM_SYSTEM_PROMPT=\"\"\"\\\\\\nYou are an advanced assistant that provides summarization given any book name\"\"\"\\n\\ntemplate=\"\"\"Use the following pieces of information to answer the user\\'s question.\\nIf you dont know the answer just say you know, don\\'t try to make up an answer.\\n\\nContext:{context}\\nQuestion:{question}\\n\\nOnly return the helpful answer below and nothing else\\nHelpful answer\\n\"\"\"\\n\\n', metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='CUSTOM_SYSTEM_PROMPT=\"\"\"\\\\\\nYou are an advanced assistant that provides summarization given any book name\"\"\"\\n', metadata={'source': 'test_repo\\\\src\\\\helpp.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from langchain import PromptTemplate\\nfrom langchain import LLMChain\\nfrom langchain.llms import CTransformers\\nfrom src.helper import *\\n\\nB_INST, E_INST = \"[INST]\", \"[/INST]\"\\nB_SYS, E_SYS = \"<<SYS>>\\\\n\", \"\\\\n<</SYS>>\\\\n\\\\n\"\\n\\n\\ninstruction = \"Summarization of the book \\\\n\\\\n {text}\"\\n\\n\\nSYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS\\ntemplate = B_INST + SYSTEM_PROMPT + instruction + E_INST\\n\\n\\nprompt = PromptTemplate(template=template, input_variables=[\"text\"])\\n\\n\\nllm = CTransformers(model=\\'model/llama-2-7b-chat.ggmlv3.q4_0.bin\\',\\n                    model_type=\\'llama\\',\\n                    config={\\'max_new_tokens\\': 128,\\n                            \\'temperature\\': 0.01}\\n                   )\\n\\n\\nLLM_Chain=LLMChain(prompt=prompt, llm=llm)\\n\\nprint(LLM_Chain.run(\\'Encyclopedia\\'))', metadata={'source': 'test_repo\\\\src\\\\run_local.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Aware chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 2000,\n",
    "                                                             chunk_overlap = 200)\n",
    "texts = documents_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "HUGGING_FACE_TOKEN = \"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\"\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"microsoft/Phi-3-mini-4k-instruct\",huggingfacehub_api_token=HUGGING_FACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\genai_llama\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vectordb\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = './data'\n",
    "vectordb = Chroma.from_documents(documents=texts,\n",
    "                                 embedding=embeddings,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving created db in disk\n",
    "\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved vectordb\n",
    "\n",
    "vectordb = Chroma(embedding_function=embeddings,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is the value of B_INST?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 7, updating n_results = 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "# This file contains the default and custom system prompt. This needs to be imported in main file to access the prompt\n",
      "\n",
      "DEFAULT_SYSTEM_PROMPT=\"\"\"\\\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
      "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
      "Please ensure that your responses are socially unbiased and positive in nature.\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of \n",
      "answering something not correct. If you don't know the answer to a question,\n",
      "please don't share false information.\"\"\"\n",
      "\n",
      "CUSTOM_SYSTEM_PROMPT=\"\"\"\\\n",
      "You are an advanced assistant that provides summarization given any book name\"\"\"\n",
      "\n",
      "template=\"\"\"Use the following pieces of information to answer the user's question.\n",
      "If you dont know the answer just say you know, don't try to make up an answer.\n",
      "\n",
      "Context:{context}\n",
      "Question:{question}\n",
      "\n",
      "Only return the helpful answer below and nothing else\n",
      "Helpful answer\n",
      "\"\"\"\n",
      "\n",
      "from langchain import PromptTemplate\n",
      "from langchain import LLMChain\n",
      "from langchain.llms import CTransformers\n",
      "from src.helpp import *\n",
      "\n",
      "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
      "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
      "\n",
      "\n",
      "# instruction = \"Convert the following text from English to Hindi: \\n\\n {text}\"\n",
      "instruction = \"Give a proper summary of the : \\n\\n {text}\"\n",
      "\n",
      "print(CUSTOM_SYSTEM_PROMPT)\n",
      "SYSTEM_PROMPT = B_SYS + CUSTOM_SYSTEM_PROMPT + E_SYS\n",
      "template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
      "\n",
      "\n",
      "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
      "\n",
      "\n",
      "llm = CTransformers(model='model/llama-2-7b-chat.ggmlv3.q4_0.bin',\n",
      "                    model_type='llama',\n",
      "                    config={'max_new_tokens': 128,\n",
      "                            'temperature': 0.01}\n",
      "                   )\n",
      "\n",
      "\n",
      "LLM_Chain=LLMChain(prompt=prompt, llm=llm)\n",
      "\n",
      "print(LLM_Chain.run('Encyclopedia'))\n",
      "\n",
      "import os\n",
      "from pathlib import Path\n",
      "import logging\n",
      "\n",
      "\n",
      "logging.basicConfig(level=logging.INFO, format='[%(asctime)s]: %(message)s:')\n",
      "\n",
      "\n",
      "list_of_files = [\n",
      "    \"src/__init__.py\",\n",
      "    \"src/run_local.py\",\n",
      "    \"src/helper.py\",\n",
      "    \"model/instruction.txt\",\n",
      "    \"requirements.txt\",\n",
      "    \"setup.py\",\n",
      "    \"main.py\",\n",
      "    \"research/trials.ipynb\",\n",
      "    \"src/help.py\"\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "for filepath in list_of_files:\n",
      "    filepath = Path(filepath)\n",
      "    filedir, filename = os.path.split(filepath)\n",
      "\n",
      "\n",
      "    if filedir !=\"\":\n",
      "        os.makedirs(filedir, exist_ok=True)\n",
      "        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\n",
      "\n",
      "    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n",
      "        with open(filepath, \"w\") as f:\n",
      "            pass\n",
      "            logging.info(f\"Creating empty file: {filepath}\")\n",
      "\n",
      "\n",
      "    else:\n",
      "        logging.info(f\"{filename} is already exists\")\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: \n",
      "Follow Up Input: what is the value of B_INST?\n",
      "Standalone question: \n",
      "\n",
      "Standalone question: What is the value of B_INST?\n",
      "\n",
      "\n",
      "Follow Up Input: What is the value of B_INST?\n",
      "Standalone question: \n",
      "\n",
      "Standalone question: What is the value of the B_INST variable?\n",
      "\n",
      "\n",
      "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: \n",
      "Helpful Answer: The value of B_INST is \"[INST]\".\n",
      "\n",
      "Follow Up Input: What is the value of B_INST?\n",
      "Standalone question: \n",
      "\n",
      "Standalone question: What is the value of the B_INST variable?\n",
      "\n",
      "\n",
      "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: \n",
      "Helpful Answer: The\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
